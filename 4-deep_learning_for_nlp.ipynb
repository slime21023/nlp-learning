{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning for Natural Language Modeling\n",
    "\n",
    "構建一個情感分類器，根據評論的文字內容預測該評論是正面還是負面。\n",
    "* 使用遷移學習 (從 wikitext-103 語言模型的預訓練權重開始)\n",
    "* 調整這些權重以專注於 IMDB 電影評論的語言特性\n",
    "\n",
    "### Language Models (語言模型)\n",
    "語言模型（Language Model, LM） 是一種基於統計或深度學習的方法，用來理解和生成自然語言的工具。它的主要目標是 估算一段文字的語言結構和語意，並預測文字序列中下一個最可能出現的單詞或詞彙。語言模型在自然語言處理（NLP）中是核心技術，廣泛應用於機器翻譯、文本生成、語音識別和對話系統等領域。\n",
    "\n",
    "語言模型的核心概念\n",
    "* 機率分佈\n",
    "* 條件機率\n",
    "\n",
    "語言模型的類型\n",
    "1. 統計語言模型：\n",
    "   * 基於統計方法，例如 n-gram 模型。\n",
    "   * 使用過去的文字序列來計算下一個單詞的機率。\n",
    "   * 缺點：需要大量數據，且難以處理長距依賴。\n",
    "2. 神經語言模型：\n",
    "   * 基於深度學習，例如 RNN、LSTM、Transformer 等架構。\n",
    "   * 能夠捕捉更複雜的語言結構。\n",
    "   * 代表性模型：GPT、BERT、T5 等。\n",
    "\n",
    "語言模型的應用\n",
    "* 文本生成： 自動生成文章、故事、詩歌等內容。\n",
    "* 機器翻譯： 將一種語言翻譯成另一種語言。\n",
    "* 語音識別與輸入法： 幫助語音識別系統理解語句，或在輸入法中進行智能聯想和補全。\n",
    "* 對話系統： 構建聊天機器人，能理解上下文並生成合理的回應。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "研究科學家 Janelle Shane 在她的[部落格](https://aiweirdness.com/)和[推特](https://twitter.com/JanelleCShane)上分享了有關創意人工智慧的探索，主要涉及文字處理。\n",
    "* [Why did the neural network cross the road?](https://aiweirdness.com/post/174691534037/why-did-the-neural-network-cross-the-road)\n",
    "* [Try these neural network-generated recipes at your own risk.](https://aiweirdness.com/post/163878889437/try-these-neural-network-generated-recipes-at-your)\n",
    "* [D&D character bios - now making slightly more sense](https://aiweirdness.com/post/183471928977/dd-character-bios-now-making-slightly-more)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 斷詞（Tokenization）\n",
    "\n",
    "斷詞是文本處理的第一步，目的是將原始句子分割成單詞，或更準確地說，是分割成「詞元（tokens）」。最簡單的方法是根據空格分割字符串，但我們可以採用更聰明的方法進行處理：\n",
    "1. 處理標點符號： 標點符號需要特別處理，不能直接與單詞混在一起。\n",
    "2. 處理縮寫詞： 一些單詞是由兩個不同單詞縮寫而成，例如 \"isn't\" 或 \"don't\"，需要將其拆分為 \"is\" 和 \"n't\"。\n",
    "3. 清理文本內容： 如果文本中包含 HTML 代碼等不必要的內容，需要進行清理。\n",
    "\n",
    "斷詞是自然語言處理中的關鍵步驟，通過智能化的處理方式，不僅能有效分割文本，還能處理標點符號、縮寫、清理雜訊，並引入特殊詞元來提升文本的結構化和可讀性。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數值化 ( Numericalization )\n",
    "\n",
    "功能:\n",
    "* Numericalization（數值化）是一種將文字資料轉換為數字形式的過程，主要用於自然語言處理（NLP）中的機器學習模型。\n",
    "* 文字本身對機器無法直接理解，因此需要將文字轉換為數字表示，便於模型進行計算和分析。\n",
    "\n",
    "步驟：\n",
    "1. 分詞與詞元化（Tokenization）: 將文本拆分為更小的單位（例如單字、詞組或子詞），這些單位稱為「詞元」（tokens）。\n",
    "2. 建立詞彙表（Vocabulary）: 創建一個詞彙表，包含文本中出現的所有詞元，並為每個詞元分配一個唯一的整數編碼。\n",
    "3. 過濾與限制: 為了控制詞彙表的大小，可能會設定一些規則，例如只保留出現頻率超過一定次數的詞元，並用特殊詞元（如 \"UNK\" 表示未知詞元）替換那些未達標的詞元。\n",
    "4. 轉換為數字序列: 將文本中的詞元替換為對應的整數編碼，從而將文字資料轉換為數字序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 遷移學習 ( Transfer Learning )\n",
    "\n",
    "Transfer Learning（遷移學習）是一種機器學習技術，指的是將在一個任務上訓練好的模型知識，應用到另一個相關任務中的方法。這種技術特別適合在資料有限或訓練資源不足的情況下使用，因為它能夠充分利用已經訓練好的模型，避免從頭開始訓練。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 藉由 Transfer Learning 建構 IMDb 語言模型\n",
    "* pretrained weights from the `wikitext-103` language model.\n",
    "* 利用該模型對英文的「知識」來構建分類器，但需要先對模型進行微調（fine-tuning），使其適應 IMDB 評論的資料集\n",
    "\n",
    "#### WikiText-103 \n",
    "* WikiText-103 是一個專為語言建模設計的資料集\n",
    "* 內容來自 Wikipedia 中經過驗證的優質文章和特色文章，這些文章的品質較高，語言結構清晰且內容豐富。\n",
    "* WikiText-103 包含超過 1 億個詞元（tokens），是 WikiText 系列中最大的版本。\n",
    "* 資料集由完整的文章組成，而非單一的獨立句子或段落，這使得它特別適合用於需要處理長期依賴性的語言模型。\n",
    "* 保留了語言的自然結構（如大小寫、標點符號等），有助於模型更準確地學習語言特性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 確保 GPU 可用\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 加載 IMDb資料集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\anaconda\\envs\\py312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "訓練集樣本數: 25000\n",
      "測試集樣本數: 25000\n",
      "前3個訓練樣本: ['I rented I AM CURIOUS-YELLOW from my video store because of all the controversy that surrounded it when it was first released in 1967. I also heard that at first it was seized by U.S. customs if it ever tried to enter this country, therefore being a fan of films considered \"controversial\" I really had to see this for myself.<br /><br />The plot is centered around a young Swedish drama student named Lena who wants to learn everything she can about life. In particular she wants to focus her attentions to making some sort of documentary on what the average Swede thought about certain political issues such as the Vietnam War and race issues in the United States. In between asking politicians and ordinary denizens of Stockholm about their opinions on politics, she has sex with her drama teacher, classmates, and married men.<br /><br />What kills me about I AM CURIOUS-YELLOW is that 40 years ago, this was considered pornographic. Really, the sex and nudity scenes are few and far between, even then it\\'s not shot like some cheaply made porno. While my countrymen mind find it shocking, in reality sex and nudity are a major staple in Swedish cinema. Even Ingmar Bergman, arguably their answer to good old boy John Ford, had sex scenes in his films.<br /><br />I do commend the filmmakers for the fact that any sex shown in the film is shown for artistic purposes rather than just to shock people and make money to be shown in pornographic theaters in America. I AM CURIOUS-YELLOW is a good film for anyone wanting to study the meat and potatoes (no pun intended) of Swedish cinema. But really, this film doesn\\'t have much of a plot.', '\"I Am Curious: Yellow\" is a risible and pretentious steaming pile. It doesn\\'t matter what one\\'s political views are because this film can hardly be taken seriously on any level. As for the claim that frontal male nudity is an automatic NC-17, that isn\\'t true. I\\'ve seen R-rated films with male nudity. Granted, they only offer some fleeting views, but where are the R-rated films with gaping vulvas and flapping labia? Nowhere, because they don\\'t exist. The same goes for those crappy cable shows: schlongs swinging in the breeze but not a clitoris in sight. And those pretentious indie movies like The Brown Bunny, in which we\\'re treated to the site of Vincent Gallo\\'s throbbing johnson, but not a trace of pink visible on Chloe Sevigny. Before crying (or implying) \"double-standard\" in matters of nudity, the mentally obtuse should take into account one unavoidably obvious anatomical difference between men and women: there are no genitals on display when actresses appears nude, and the same cannot be said for a man. In fact, you generally won\\'t see female genitals in an American film in anything short of porn or explicit erotica. This alleged double-standard is less a double standard than an admittedly depressing ability to come to terms culturally with the insides of women\\'s bodies.', \"If only to avoid making this type of film in the future. This film is interesting as an experiment but tells no cogent story.<br /><br />One might feel virtuous for sitting thru it because it touches on so many IMPORTANT issues but it does so without any discernable motive. The viewer comes away with no new perspectives (unless one comes up with one while one's mind wanders, as it will invariably do during this pointless film).<br /><br />One might better spend one's time staring out a window at a tree growing.<br /><br />\"]\n",
      "前3個訓練標籤: [0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# load the IMDB dataset\n",
    "dataset = load_dataset(\"imdb\")\n",
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "test_texts = dataset[\"test\"][\"text\"]\n",
    "train_labels = dataset[\"train\"][\"label\"]\n",
    "test_labels = dataset[\"test\"][\"label\"]\n",
    "\n",
    "# 打印部分數據\n",
    "print(\"訓練集樣本數:\", len(train_texts))\n",
    "print(\"測試集樣本數:\", len(test_texts))\n",
    "print(\"前3個訓練樣本:\", train_texts[:3])\n",
    "print(\"前3個訓練標籤:\", train_labels[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 數據預處理\n",
    "\n",
    "將 IMDb 數據集轉換為模型可接受的格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "數據預處理完成，訓練集和測試集已轉換為可用格式。\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import InputExample\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 將數據轉換為 Sentence Transformers 的 InputExample 格式\n",
    "train_examples = [InputExample(texts=[text], label=float(label)) for text, label in zip(train_texts, train_labels)]\n",
    "test_examples = [InputExample(texts=[text], label=float(label)) for text, label in zip(test_texts, test_labels)]\n",
    "\n",
    "\n",
    "# 創建 DataLoader\n",
    "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
    "test_dataloader = DataLoader(test_examples, shuffle=False, batch_size=16)\n",
    "\n",
    "print(\"數據預處理完成，訓練集和測試集已轉換為可用格式。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 微調語言模型\n",
    "\n",
    "使用 Hugging Face 的 `Trainer` 進行模型的微調。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 14\u001b[0m\n\u001b[0;32m     11\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m*\u001b[39m num_epochs \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.1\u001b[39m)  \u001b[38;5;66;03m# 10% 的 warmup\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 微調模型\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     15\u001b[0m     train_objectives\u001b[38;5;241m=\u001b[39m[(train_dataloader, train_loss)],\n\u001b[0;32m     16\u001b[0m     epochs\u001b[38;5;241m=\u001b[39mnum_epochs,\n\u001b[0;32m     17\u001b[0m     warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps,\n\u001b[0;32m     18\u001b[0m     output_path\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel/finetuned_imdb_model\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     19\u001b[0m )\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m模型微調完成，已保存至 \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./finetuned_imdb_model\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\py312\\Lib\\site-packages\\sentence_transformers\\fit_mixin.py:385\u001b[0m, in \u001b[0;36mFitMixin.fit\u001b[1;34m(self, train_objectives, evaluator, epochs, steps_per_epoch, scheduler, warmup_steps, optimizer_class, optimizer_params, weight_decay, evaluation_steps, output_path, save_best_model, max_grad_norm, use_amp, callback, show_progress_bar, checkpoint_path, checkpoint_save_steps, checkpoint_save_total_limit)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m output_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    383\u001b[0m     trainer\u001b[38;5;241m.\u001b[39madd_callback(SaveModelCallback(output_path, evaluator, save_best_model))\n\u001b[1;32m--> 385\u001b[0m trainer\u001b[38;5;241m.\u001b[39mtrain()\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\py312\\Lib\\site-packages\\transformers\\trainer.py:2241\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2239\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2240\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2241\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2242\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2243\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2244\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2245\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2246\u001b[0m     )\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\py312\\Lib\\site-packages\\transformers\\trainer.py:2548\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2541\u001b[0m context \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   2542\u001b[0m     functools\u001b[38;5;241m.\u001b[39mpartial(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mno_sync, model\u001b[38;5;241m=\u001b[39mmodel)\n\u001b[0;32m   2543\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(batch_samples) \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2544\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m!=\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2545\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m contextlib\u001b[38;5;241m.\u001b[39mnullcontext\n\u001b[0;32m   2546\u001b[0m )\n\u001b[0;32m   2547\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[1;32m-> 2548\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs, num_items_in_batch)\n\u001b[0;32m   2550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2551\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   2552\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[0;32m   2553\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   2554\u001b[0m ):\n\u001b[0;32m   2555\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   2556\u001b[0m     tr_loss \u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m+\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\py312\\Lib\\site-packages\\transformers\\trainer.py:3698\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m   3695\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb\u001b[38;5;241m.\u001b[39mreduce_mean()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m   3697\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 3698\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_loss(model, inputs, num_items_in_batch\u001b[38;5;241m=\u001b[39mnum_items_in_batch)\n\u001b[0;32m   3700\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[0;32m   3701\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   3702\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   3703\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mtorch_empty_cache_steps \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   3704\u001b[0m ):\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\py312\\Lib\\site-packages\\sentence_transformers\\trainer.py:405\u001b[0m, in \u001b[0;36mSentenceTransformerTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[0m\n\u001b[0;32m    398\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    399\u001b[0m     model \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped\n\u001b[0;32m    400\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m model \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel  \u001b[38;5;66;03m# Only if the model is wrapped\u001b[39;00m\n\u001b[0;32m    401\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(loss_fn, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Only if the loss stores the model\u001b[39;00m\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m loss_fn\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m!=\u001b[39m model  \u001b[38;5;66;03m# Only if the wrapped model is not already stored\u001b[39;00m\n\u001b[0;32m    403\u001b[0m ):\n\u001b[0;32m    404\u001b[0m     loss_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moverride_model_in_loss(loss_fn, model)\n\u001b[1;32m--> 405\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_fn(features, labels)\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_outputs:\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;66;03m# During prediction/evaluation, `compute_loss` will be called with `return_outputs=True`.\u001b[39;00m\n\u001b[0;32m    408\u001b[0m     \u001b[38;5;66;03m# However, Sentence Transformer losses do not return outputs, so we return an empty dictionary.\u001b[39;00m\n\u001b[0;32m    409\u001b[0m     \u001b[38;5;66;03m# This does not result in any problems, as the SentenceTransformerTrainingArguments sets\u001b[39;00m\n\u001b[0;32m    410\u001b[0m     \u001b[38;5;66;03m# `prediction_loss_only=True` which means that the output is not used.\u001b[39;00m\n\u001b[0;32m    411\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss, {}\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\py312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\anaconda\\envs\\py312\\Lib\\site-packages\\sentence_transformers\\losses\\CosineSimilarityLoss.py:80\u001b[0m, in \u001b[0;36mCosineSimilarityLoss.forward\u001b[1;34m(self, sentence_features, labels)\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, sentence_features: Iterable[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Tensor]], labels: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m     79\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel(sentence_feature)[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentence_embedding\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m sentence_feature \u001b[38;5;129;01min\u001b[39;00m sentence_features]\n\u001b[1;32m---> 80\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcos_score_transformation(torch\u001b[38;5;241m.\u001b[39mcosine_similarity(embeddings[\u001b[38;5;241m0\u001b[39m], embeddings[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss_fct(output, labels\u001b[38;5;241m.\u001b[39mfloat()\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, losses\n",
    "\n",
    "# 加載 Sentence Transformer 模型\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device=device)\n",
    "\n",
    "# 定義損失函數\n",
    "train_loss = losses.CosineSimilarityLoss(model)\n",
    "\n",
    "# 訓練參數\n",
    "num_epochs = 1\n",
    "warmup_steps = int(len(train_dataloader) * num_epochs * 0.1)  # 10% 的 warmup\n",
    "\n",
    "# 微調模型\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    epochs=num_epochs,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path='model/finetuned_imdb_model'\n",
    ")\n",
    "\n",
    "print(\"模型微調完成，已保存至 './finetuned_imdb_model'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 評估模型效能"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import evaluation\n",
    "\n",
    "# 加載微調後的模型\n",
    "finetuned_model = SentenceTransformer('./finetuned_imdb_model', device=device)\n",
    "\n",
    "# 定義評估器\n",
    "evaluator = evaluation.BinaryClassificationEvaluator(\n",
    "    sentences1=test_texts[:1000],\n",
    "    sentences2=test_texts[:1000],\n",
    "    scores=[float(label) for label in test_labels[:1000]]\n",
    ")\n",
    "\n",
    "# 執行評估\n",
    "evaluation_result = evaluator(finetuned_model)\n",
    "print(\"模型效能評估結果:\", evaluation_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
